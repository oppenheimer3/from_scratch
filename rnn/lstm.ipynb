{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3yyGgR_GpZYu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lksWE6kGEmNZ",
        "outputId": "16fed36f-25ff-45b3-e99c-13771eb8ed86"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "seeHf7X-Dmrc"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "rK-9kxf_5GeA"
      },
      "outputs": [],
      "source": [
        "temperatures = [\n",
        "    72, 73, 75, 78, 80, 82, 85, 87, 88, 85,\n",
        "    81, 78, 76, 74, 73, 72, 71, 72, 74, 76,\n",
        "    79, 82, 85, 88, 90, 92, 93, 92, 89, 86\n",
        "]\n",
        "data = []\n",
        "labels = []\n",
        "for t in range(26):\n",
        "    data.append([[temperatures[t+i]]for i in range(4)])\n",
        "    labels.append(temperatures[t+4])\n",
        "data = torch.tensor(data, dtype=torch.float32)\n",
        "targets = torch.tensor(labels, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "_HGSBvI-y-nl"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, l, hl,ol):\n",
        "    super().__init__()\n",
        "    self.sig = nn.Sigmoid()\n",
        "    self.U = nn.Linear(l, hl)\n",
        "    self.W = nn.Linear(hl, hl)\n",
        "    self.Ug = nn.Linear(l,hl)\n",
        "    self.Wg = nn.Linear(hl,hl)\n",
        "    self.Uf = nn.Linear(l,hl)\n",
        "    self.Wf = nn.Linear(hl,hl)\n",
        "    self.Uo = nn.Linear(l,hl)\n",
        "    self.Wo = nn.Linear(hl,hl)\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.fc = nn.Linear(hl, ol)\n",
        "  def forward(self, x, st_1, ht_1):\n",
        "    xt = self.sig(self.U(x) + self.W(ht_1))\n",
        "    g = self.sig(self.Ug(x) + self.Wg(ht_1))\n",
        "    f = self.sig(self.Uf(x) + self.Wf(ht_1))\n",
        "    q = self.sig(self.Uo(x) + self.Wo(ht_1))\n",
        "    s = f * st_1 + g * xt\n",
        "    h = self.tanh(s) * q\n",
        "    o = self.fc(h)\n",
        "    return s, h, o\n",
        "\n",
        "class UnfoldLSTM(nn.Module):\n",
        "  def __init__(self, l, hl,ol) -> None:\n",
        "     super().__init__()\n",
        "     self.hl = hl\n",
        "     self.lstm = LSTM( l, hl,ol)\n",
        "  def forward(self, x):\n",
        "    x = torch.permute(x, (1,0,2))\n",
        "    st_1 , ht_1 = torch.zeros( self.hl).to(device), torch.zeros( self.hl).to(device)\n",
        "    for i in x:\n",
        "      st_1, ht_1, o = self.lstm(i, st_1, ht_1)\n",
        "    return o\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKO5YyNURbZL",
        "outputId": "538eefd5-60eb-444b-de78-d846a0d348dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [100/10000], Loss: 2958.4814\n",
            "Epoch [200/10000], Loss: 937.1837\n",
            "Epoch [300/10000], Loss: 263.9018\n",
            "Epoch [400/10000], Loss: 86.6648\n",
            "Epoch [500/10000], Loss: 51.9279\n",
            "Epoch [600/10000], Loss: 46.9428\n",
            "Epoch [700/10000], Loss: 46.4206\n",
            "Epoch [800/10000], Loss: 46.3799\n",
            "Epoch [900/10000], Loss: 32.1518\n",
            "Epoch [1000/10000], Loss: 21.3116\n",
            "Epoch [1100/10000], Loss: 10.7093\n",
            "Epoch [1200/10000], Loss: 5.5106\n",
            "Epoch [1300/10000], Loss: 3.6298\n",
            "Epoch [1400/10000], Loss: 2.5596\n",
            "Epoch [1500/10000], Loss: 1.9128\n",
            "Epoch [1600/10000], Loss: 1.5194\n",
            "Epoch [1700/10000], Loss: 1.2931\n",
            "Epoch [1800/10000], Loss: 1.1577\n",
            "Epoch [1900/10000], Loss: 1.0631\n",
            "Epoch [2000/10000], Loss: 0.9961\n",
            "Epoch [2100/10000], Loss: 0.9413\n",
            "Epoch [2200/10000], Loss: 0.8539\n",
            "Epoch [2300/10000], Loss: 0.7425\n",
            "Epoch [2400/10000], Loss: 0.7042\n",
            "Epoch [2500/10000], Loss: 0.6905\n",
            "Epoch [2600/10000], Loss: 0.6468\n",
            "Epoch [2700/10000], Loss: 0.5918\n",
            "Epoch [2800/10000], Loss: 0.5773\n",
            "Epoch [2900/10000], Loss: 0.5515\n",
            "Epoch [3000/10000], Loss: 0.4869\n",
            "Epoch [3100/10000], Loss: 0.4753\n",
            "Epoch [3200/10000], Loss: 0.4294\n",
            "Epoch [3300/10000], Loss: 0.3948\n",
            "Epoch [3400/10000], Loss: 0.3626\n",
            "Epoch [3500/10000], Loss: 0.3405\n",
            "Epoch [3600/10000], Loss: 0.3182\n",
            "Epoch [3700/10000], Loss: 0.2977\n",
            "Epoch [3800/10000], Loss: 0.2783\n",
            "Epoch [3900/10000], Loss: 0.2556\n",
            "Epoch [4000/10000], Loss: 0.2610\n",
            "Epoch [4100/10000], Loss: 0.3147\n",
            "Epoch [4200/10000], Loss: 0.2122\n",
            "Epoch [4300/10000], Loss: 0.2015\n",
            "Epoch [4400/10000], Loss: 0.1739\n",
            "Epoch [4500/10000], Loss: 0.2396\n",
            "Epoch [4600/10000], Loss: 0.1577\n",
            "Epoch [4700/10000], Loss: 0.1495\n",
            "Epoch [4800/10000], Loss: 0.2176\n",
            "Epoch [4900/10000], Loss: 0.1379\n",
            "Epoch [5000/10000], Loss: 0.1279\n",
            "Epoch [5100/10000], Loss: 0.1368\n",
            "Epoch [5200/10000], Loss: 0.1157\n",
            "Epoch [5300/10000], Loss: 0.1184\n",
            "Epoch [5400/10000], Loss: 0.1115\n",
            "Epoch [5500/10000], Loss: 0.1379\n",
            "Epoch [5600/10000], Loss: 0.1672\n",
            "Epoch [5700/10000], Loss: 0.0931\n",
            "Epoch [5800/10000], Loss: 0.0892\n",
            "Epoch [5900/10000], Loss: 0.0906\n",
            "Epoch [6000/10000], Loss: 0.1216\n",
            "Epoch [6100/10000], Loss: 0.0800\n",
            "Epoch [6200/10000], Loss: 0.0888\n",
            "Epoch [6300/10000], Loss: 0.1021\n",
            "Epoch [6400/10000], Loss: 0.0787\n",
            "Epoch [6500/10000], Loss: 0.0789\n",
            "Epoch [6600/10000], Loss: 0.0639\n",
            "Epoch [6700/10000], Loss: 0.0608\n",
            "Epoch [6800/10000], Loss: 0.1234\n",
            "Epoch [6900/10000], Loss: 0.0690\n",
            "Epoch [7000/10000], Loss: 0.0791\n",
            "Epoch [7100/10000], Loss: 0.0546\n",
            "Epoch [7200/10000], Loss: 0.0755\n",
            "Epoch [7300/10000], Loss: 0.0498\n",
            "Epoch [7400/10000], Loss: 0.0430\n",
            "Epoch [7500/10000], Loss: 0.0536\n",
            "Epoch [7600/10000], Loss: 0.0467\n",
            "Epoch [7700/10000], Loss: 0.0499\n",
            "Epoch [7800/10000], Loss: 0.0387\n",
            "Epoch [7900/10000], Loss: 0.0346\n",
            "Epoch [8000/10000], Loss: 0.0619\n",
            "Epoch [8100/10000], Loss: 0.0342\n",
            "Epoch [8200/10000], Loss: 0.0346\n",
            "Epoch [8300/10000], Loss: 0.0328\n",
            "Epoch [8400/10000], Loss: 0.0471\n",
            "Epoch [8500/10000], Loss: 0.0642\n",
            "Epoch [8600/10000], Loss: 0.0287\n",
            "Epoch [8700/10000], Loss: 0.0855\n",
            "Epoch [8800/10000], Loss: 0.0341\n",
            "Epoch [8900/10000], Loss: 0.0363\n",
            "Epoch [9000/10000], Loss: 0.0311\n",
            "Epoch [9100/10000], Loss: 0.0266\n",
            "Epoch [9200/10000], Loss: 0.0245\n",
            "Epoch [9300/10000], Loss: 0.0281\n",
            "Epoch [9400/10000], Loss: 0.0411\n",
            "Epoch [9500/10000], Loss: 0.0189\n",
            "Epoch [9600/10000], Loss: 0.0432\n",
            "Epoch [9700/10000], Loss: 0.0173\n",
            "Epoch [9800/10000], Loss: 0.0215\n",
            "Epoch [9900/10000], Loss: 0.0238\n",
            "Epoch [10000/10000], Loss: 0.0355\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[73.2115],\n",
              "        [81.9981],\n",
              "        [80.2234],\n",
              "        [76.2262],\n",
              "        [89.1399],\n",
              "        [86.0084],\n",
              "        [73.9787],\n",
              "        [87.9834],\n",
              "        [87.2624],\n",
              "        [77.9816],\n",
              "        [90.2274],\n",
              "        [82.2673],\n",
              "        [85.2137],\n",
              "        [72.1611],\n",
              "        [81.0352],\n",
              "        [73.9765],\n",
              "        [76.0066],\n",
              "        [91.8482],\n",
              "        [72.0492],\n",
              "        [88.2386],\n",
              "        [85.5061],\n",
              "        [85.1229],\n",
              "        [91.8761],\n",
              "        [78.9166],\n",
              "        [93.3143],\n",
              "        [71.1283]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "batch_size = 26  # You can adjust this as needed\n",
        "# Define hyperparameters\n",
        "input_size = 1  # Input size\n",
        "hidden_size = 128  # Hidden layer size\n",
        "output_size = 1  # Output size\n",
        "sequence_length = 4  # Length of input sequences\n",
        "num_epochs = 10000\n",
        "learning_rate = 0.01\n",
        "\n",
        "dataset = TensorDataset(data, targets)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "# Instantiate the LSTM model\n",
        "model = UnfoldLSTM(input_size, hidden_size, output_size)\n",
        "model.to(device)\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs , labels = inputs.to(device), labels.to(device)\n",
        "        predictions = model(inputs)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(predictions.view(-1), labels)\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W48xxT1W8syI",
        "outputId": "444d6ba6-9c89-464b-e117-c024b6a8a177"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([73., 82., 80., 76., 89., 86., 74., 88., 87., 78., 90., 82., 85., 72.,\n",
              "        81., 74., 76., 92., 72., 88., 85., 85., 92., 79., 93., 71.])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRGIZmJjFyew",
        "outputId": "ba463626-fd81-4ab5-f26d-884b798072c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10000], Loss: 6745.9253\n",
            "Epoch [101/10000], Loss: 5055.7319\n",
            "Epoch [201/10000], Loss: 4137.6597\n",
            "Epoch [301/10000], Loss: 3385.8992\n",
            "Epoch [401/10000], Loss: 2753.6550\n",
            "Epoch [501/10000], Loss: 2221.2544\n",
            "Epoch [601/10000], Loss: 1775.2551\n",
            "Epoch [701/10000], Loss: 1404.6755\n",
            "Epoch [801/10000], Loss: 1099.8604\n",
            "Epoch [901/10000], Loss: 852.0562\n",
            "Epoch [1001/10000], Loss: 653.2350\n",
            "Epoch [1101/10000], Loss: 496.0274\n",
            "Epoch [1201/10000], Loss: 373.7024\n",
            "Epoch [1301/10000], Loss: 280.1738\n",
            "Epoch [1401/10000], Loss: 210.0104\n",
            "Epoch [1501/10000], Loss: 158.4461\n",
            "Epoch [1601/10000], Loss: 121.3793\n",
            "Epoch [1701/10000], Loss: 95.3571\n",
            "Epoch [1801/10000], Loss: 77.5438\n",
            "Epoch [1901/10000], Loss: 65.6722\n",
            "Epoch [2001/10000], Loss: 57.9818\n",
            "Epoch [2101/10000], Loss: 53.1472\n",
            "Epoch [2201/10000], Loss: 50.2022\n",
            "Epoch [2301/10000], Loss: 48.4668\n",
            "Epoch [2401/10000], Loss: 47.4792\n",
            "Epoch [2501/10000], Loss: 46.9375\n",
            "Epoch [2601/10000], Loss: 46.6515\n",
            "Epoch [2701/10000], Loss: 46.5065\n",
            "Epoch [2801/10000], Loss: 46.4360\n",
            "Epoch [2901/10000], Loss: 46.4033\n",
            "Epoch [3001/10000], Loss: 46.3887\n",
            "Epoch [3101/10000], Loss: 46.3826\n",
            "Epoch [3201/10000], Loss: 46.3801\n",
            "Epoch [3301/10000], Loss: 46.3792\n",
            "Epoch [3401/10000], Loss: 46.3788\n",
            "Epoch [3501/10000], Loss: 46.3787\n",
            "Epoch [3601/10000], Loss: 46.3787\n",
            "Epoch [3701/10000], Loss: 46.3787\n",
            "Epoch [3801/10000], Loss: 46.3787\n",
            "Epoch [3901/10000], Loss: 46.3787\n",
            "Epoch [4001/10000], Loss: 46.3787\n",
            "Epoch [4101/10000], Loss: 46.3787\n",
            "Epoch [4201/10000], Loss: 46.3787\n",
            "Epoch [4301/10000], Loss: 46.3787\n",
            "Epoch [4401/10000], Loss: 46.3787\n",
            "Epoch [4501/10000], Loss: 46.3787\n",
            "Epoch [4601/10000], Loss: 46.3787\n",
            "Epoch [4701/10000], Loss: 46.3787\n",
            "Epoch [4801/10000], Loss: 46.3787\n",
            "Epoch [4901/10000], Loss: 46.3787\n",
            "Epoch [5001/10000], Loss: 46.3787\n",
            "Epoch [5101/10000], Loss: 46.3787\n",
            "Epoch [5201/10000], Loss: 46.3787\n",
            "Epoch [5301/10000], Loss: 46.3787\n",
            "Epoch [5401/10000], Loss: 46.3787\n",
            "Epoch [5501/10000], Loss: 46.3787\n",
            "Epoch [5601/10000], Loss: 46.3787\n",
            "Epoch [5701/10000], Loss: 46.3787\n",
            "Epoch [5801/10000], Loss: 46.3787\n",
            "Epoch [5901/10000], Loss: 46.3787\n",
            "Epoch [6001/10000], Loss: 46.3787\n",
            "Epoch [6101/10000], Loss: 46.3787\n",
            "Epoch [6201/10000], Loss: 46.3787\n",
            "Epoch [6301/10000], Loss: 46.3786\n",
            "Epoch [6401/10000], Loss: 46.3786\n",
            "Epoch [6501/10000], Loss: 46.3779\n",
            "Epoch [6601/10000], Loss: 46.1201\n",
            "Epoch [6701/10000], Loss: 15.0794\n",
            "Epoch [6801/10000], Loss: 5.0778\n",
            "Epoch [6901/10000], Loss: 3.2569\n",
            "Epoch [7001/10000], Loss: 2.6903\n",
            "Epoch [7101/10000], Loss: 1.9019\n",
            "Epoch [7201/10000], Loss: 1.5797\n",
            "Epoch [7301/10000], Loss: 1.2643\n",
            "Epoch [7401/10000], Loss: 1.3461\n",
            "Epoch [7501/10000], Loss: 1.0041\n",
            "Epoch [7601/10000], Loss: 0.9171\n",
            "Epoch [7701/10000], Loss: 0.9299\n",
            "Epoch [7801/10000], Loss: 0.9472\n",
            "Epoch [7901/10000], Loss: 0.9841\n",
            "Epoch [8001/10000], Loss: 0.8883\n",
            "Epoch [8101/10000], Loss: 0.7869\n",
            "Epoch [8201/10000], Loss: 0.8708\n",
            "Epoch [8301/10000], Loss: 0.7280\n",
            "Epoch [8401/10000], Loss: 0.6833\n",
            "Epoch [8501/10000], Loss: 0.6382\n",
            "Epoch [8601/10000], Loss: 1.1435\n",
            "Epoch [8701/10000], Loss: 1.2607\n",
            "Epoch [8801/10000], Loss: 0.8102\n",
            "Epoch [8901/10000], Loss: 0.5982\n",
            "Epoch [9001/10000], Loss: 0.5835\n",
            "Epoch [9101/10000], Loss: 0.5982\n",
            "Epoch [9201/10000], Loss: 0.5743\n",
            "Epoch [9301/10000], Loss: 2.1762\n",
            "Epoch [9401/10000], Loss: 0.6290\n",
            "Epoch [9501/10000], Loss: 0.5621\n",
            "Epoch [9601/10000], Loss: 0.7218\n",
            "Epoch [9701/10000], Loss: 0.6110\n",
            "Epoch [9801/10000], Loss: 0.5657\n",
            "Epoch [9901/10000], Loss: 0.5563\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Assuming you have NumPy arrays for your data and targets\n",
        "import numpy as np\n",
        "# Assuming data and targets are NumPy arrays\n",
        "# data = np.random.rand(26, 4, 1)  # Replace this with your actual data\n",
        "# targets = np.random.rand(26)    # Replace this with your actual targets\n",
        "\n",
        "# data = torch.tensor(data, dtype=torch.float32)\n",
        "# targets = torch.tensor(targets, dtype=torch.float32)\n",
        "dataset = TensorDataset(data, targets)\n",
        "batch_size = 26  # You can adjust this as needed\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])  # Use the last time step's output\n",
        "        return out\n",
        "\n",
        "# Define the model's hyperparameters\n",
        "input_size = 1\n",
        "hidden_size = 64  # You can adjust this as needed\n",
        "num_layers = 2    # You can adjust this as needed\n",
        "output_size = 1\n",
        "\n",
        "model = LSTMModel(input_size, hidden_size, num_layers, output_size).to(device)\n",
        "criterion = nn.MSELoss()  # Mean squared error loss for regression\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # You can adjust the learning rate\n",
        "num_epochs = 10000  # You can adjust the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs , labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    if epoch % 100 ==0:\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}